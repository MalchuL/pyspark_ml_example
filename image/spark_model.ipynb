{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from pyspark import SparkContext, SparkConf,StorageLevel\n",
    "# create sparksession\n",
    "conf = SparkConf().setAppName(\"MLLab\").setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = \"../data/features.csv\"\n",
    "labels_path = \"../data/labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = spark.read.format(\"csv\").option(\"inferSchema\", 'true').option(\"header\", \"true\").load(features_path)\n",
    "labels = spark.read.format(\"csv\").option(\"inferSchema\", 'true').option(\"header\", \"true\").load(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'amount_tsh',\n",
       " 'date_recorded',\n",
       " 'funder',\n",
       " 'gps_height',\n",
       " 'installer',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'wpt_name',\n",
       " 'num_private',\n",
       " 'basin',\n",
       " 'subvillage',\n",
       " 'region',\n",
       " 'region_code',\n",
       " 'district_code',\n",
       " 'lga',\n",
       " 'ward',\n",
       " 'population',\n",
       " 'public_meeting',\n",
       " 'recorded_by',\n",
       " 'scheme_management',\n",
       " 'scheme_name',\n",
       " 'permit',\n",
       " 'construction_year',\n",
       " 'extraction_type',\n",
       " 'extraction_type_group',\n",
       " 'extraction_type_class',\n",
       " 'management',\n",
       " 'management_group',\n",
       " 'payment',\n",
       " 'payment_type',\n",
       " 'water_quality',\n",
       " 'quality_group',\n",
       " 'quantity',\n",
       " 'quantity_group',\n",
       " 'source',\n",
       " 'source_type',\n",
       " 'source_class',\n",
       " 'waterpoint_type',\n",
       " 'waterpoint_type_group']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "full_train = features.alias('a').join(labels.alias('b'),col('b.id') == col('a.id')).drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount_tsh=6000.0, date_recorded=datetime.datetime(2011, 3, 14, 0, 0), funder='Roman', gps_height=1390, installer='Roman', longitude=34.93809275, latitude=-9.85632177, wpt_name='none', num_private=0, basin='Lake Nyasa', subvillage='Mnyusi B', region='Iringa', region_code=11, district_code=5, lga='Ludewa', ward='Mundindi', population=109, public_meeting=True, recorded_by='GeoData Consultants Ltd', scheme_management='VWC', scheme_name='Roman', permit=False, construction_year=1999, extraction_type='gravity', extraction_type_group='gravity', extraction_type_class='gravity', management='vwc', management_group='user-group', payment='pay annually', payment_type='annually', water_quality='soft', quality_group='good', quantity='enough', quantity_group='enough', source='spring', source_type='spring', source_class='groundwater', waterpoint_type='communal standpipe', waterpoint_type_group='communal standpipe', status_group='functional')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(amount_tsh,DoubleType,true),StructField(date_recorded,TimestampType,true),StructField(funder,StringType,true),StructField(gps_height,IntegerType,true),StructField(installer,StringType,true),StructField(longitude,DoubleType,true),StructField(latitude,DoubleType,true),StructField(wpt_name,StringType,true),StructField(num_private,IntegerType,true),StructField(basin,StringType,true),StructField(subvillage,StringType,true),StructField(region,StringType,true),StructField(region_code,IntegerType,true),StructField(district_code,IntegerType,true),StructField(lga,StringType,true),StructField(ward,StringType,true),StructField(population,IntegerType,true),StructField(public_meeting,BooleanType,true),StructField(recorded_by,StringType,true),StructField(scheme_management,StringType,true),StructField(scheme_name,StringType,true),StructField(permit,BooleanType,true),StructField(construction_year,IntegerType,true),StructField(extraction_type,StringType,true),StructField(extraction_type_group,StringType,true),StructField(extraction_type_class,StringType,true),StructField(management,StringType,true),StructField(management_group,StringType,true),StructField(payment,StringType,true),StructField(payment_type,StringType,true),StructField(water_quality,StringType,true),StructField(quality_group,StringType,true),StructField(quantity,StringType,true),StructField(quantity_group,StringType,true),StructField(source,StringType,true),StructField(source_type,StringType,true),StructField(source_class,StringType,true),StructField(waterpoint_type,StringType,true),StructField(waterpoint_type_group,StringType,true),StructField(status_group,StringType,true)))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType, DateType, BooleanType\n",
    "\n",
    "double_columns = ['amount_tsh', 'longitude', 'latitude', ]\n",
    "int_columns = ['gps_height', 'num_private', 'region_code', 'district_code', 'population','construction_year']\n",
    "bool_columns = ['public_meeting','permit']\n",
    "numeric_columns = double_columns + int_columns\n",
    "date_columns = ['date_recorded']\n",
    "\n",
    "indexed_columns = date_columns + ['lga']\n",
    "drop_colums =  ['ward', 'funder', 'installer', 'wpt_name', 'subvillage', 'scheme_name', 'recorded_by']\n",
    "\n",
    "target_column = 'status_group'\n",
    "\n",
    "not_unique_columns = double_columns + int_columns + bool_columns + drop_colums + [target_column] + indexed_columns\n",
    "unique_columns = [elem for elem in full_train.columns if elem not in not_unique_columns] + bool_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = full_train#.drop('date_recorded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numeric_columns:\n",
    "    dropped =  dropped.withColumn(column, dropped[column].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in bool_columns:\n",
    "    dropped =  dropped.withColumn(column, dropped[column].cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in date_columns:\n",
    "    dropped =  dropped.withColumn(column, dropped[column].cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_columns:\n",
      "basin 9\n",
      "region 21\n",
      "scheme_management 13\n",
      "extraction_type 18\n",
      "extraction_type_group 13\n",
      "extraction_type_class 7\n",
      "management 12\n",
      "management_group 5\n",
      "payment 7\n",
      "payment_type 7\n",
      "water_quality 8\n",
      "quality_group 6\n",
      "quantity 5\n",
      "quantity_group 5\n",
      "source 10\n",
      "source_type 7\n",
      "source_class 3\n",
      "waterpoint_type 7\n",
      "waterpoint_type_group 6\n",
      "public_meeting 3\n",
      "permit 3\n",
      "\n",
      "dropped columns:\n",
      "ward 2092\n",
      "funder 1898\n",
      "installer 2146\n",
      "wpt_name 37400\n",
      "subvillage 19288\n",
      "scheme_name 2697\n",
      "recorded_by 1\n",
      "\n",
      "indexed columns:\n",
      "date_recorded 356\n",
      "lga 125\n"
     ]
    }
   ],
   "source": [
    "unique_values = {}\n",
    "print(\"unique_columns:\")\n",
    "for column in unique_columns:\n",
    "    unique_values[column] = dropped.select(column).distinct().count()\n",
    "    print(column, unique_values[column])\n",
    "print('\\ndropped columns:')\n",
    "for column in drop_colums:\n",
    "    unique_values[column] = full_train.select(column).distinct().count()\n",
    "    print(column, unique_values[column])\n",
    "print('\\nindexed columns:')\n",
    "for column in indexed_columns:\n",
    "    unique_values[column] = full_train.select(column).distinct().count()\n",
    "    print(column, unique_values[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(amount_tsh,DoubleType,true),StructField(date_recorded,StringType,true),StructField(funder,StringType,true),StructField(gps_height,DoubleType,true),StructField(installer,StringType,true),StructField(longitude,DoubleType,true),StructField(latitude,DoubleType,true),StructField(wpt_name,StringType,true),StructField(num_private,DoubleType,true),StructField(basin,StringType,true),StructField(subvillage,StringType,true),StructField(region,StringType,true),StructField(region_code,DoubleType,true),StructField(district_code,DoubleType,true),StructField(lga,StringType,true),StructField(ward,StringType,true),StructField(population,DoubleType,true),StructField(public_meeting,StringType,true),StructField(recorded_by,StringType,true),StructField(scheme_management,StringType,true),StructField(scheme_name,StringType,true),StructField(permit,StringType,true),StructField(construction_year,DoubleType,true),StructField(extraction_type,StringType,true),StructField(extraction_type_group,StringType,true),StructField(extraction_type_class,StringType,true),StructField(management,StringType,true),StructField(management_group,StringType,true),StructField(payment,StringType,true),StructField(payment_type,StringType,true),StructField(water_quality,StringType,true),StructField(quality_group,StringType,true),StructField(quantity,StringType,true),StructField(quantity_group,StringType,true),StructField(source,StringType,true),StructField(source_type,StringType,true),StructField(source_class,StringType,true),StructField(waterpoint_type,StringType,true),StructField(waterpoint_type_group,StringType,true),StructField(status_group,StringType,true)))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amount_tsh': 0.0, 'longitude': 34.87524907, 'latitude': -5.06903933, 'gps_height': 353.0, 'num_private': 0.0, 'region_code': 12.0, 'district_code': 3.0, 'population': 20.0, 'construction_year': 1985.0}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in include\n",
    "    ))\n",
    "    return df.fillna(stats.first().asDict())\n",
    "\n",
    "def fill_with_median(df, include=list()):\n",
    "    medians = df.approxQuantile(include, [0.5], 0.01)\n",
    "    assert isinstance(include, (tuple,list))\n",
    "    medians = list(map(lambda x: x[0],medians))\n",
    "    medians=dict(zip(include,medians))\n",
    "    print(medians)\n",
    "    return df.fillna(medians)\n",
    "\n",
    "\n",
    "dropped = fill_with_median(dropped, numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_column = \"features\"\n",
    "label_column = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount_tsh=6000.0, longitude=34.93809275, latitude=-9.85632177, gps_height=1390.0, num_private=0.0, region_code=11.0, district_code=5.0, population=109.0, construction_year=1999.0),\n",
       " Row(amount_tsh=0.0, longitude=34.6987661, latitude=-2.14746569, gps_height=1399.0, num_private=0.0, region_code=20.0, district_code=2.0, population=280.0, construction_year=2010.0),\n",
       " Row(amount_tsh=25.0, longitude=37.46066446, latitude=-3.82132853, gps_height=686.0, num_private=0.0, region_code=21.0, district_code=4.0, population=250.0, construction_year=2009.0),\n",
       " Row(amount_tsh=0.0, longitude=38.48616088, latitude=-11.15529772, gps_height=263.0, num_private=0.0, region_code=90.0, district_code=63.0, population=58.0, construction_year=1986.0),\n",
       " Row(amount_tsh=0.0, longitude=31.13084671, latitude=-1.82535885, gps_height=0.0, num_private=0.0, region_code=18.0, district_code=1.0, population=0.0, construction_year=0.0)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.select(numeric_columns).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|status_group|\n",
      "+------------+\n",
      "|           0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropped.schema\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "temp_df = dropped.select([target_column])\n",
    "temp_df.select([count(when(isnan(c), c)).alias(c) for c in temp_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount_tsh',\n",
       " 'date_recorded',\n",
       " 'funder',\n",
       " 'gps_height',\n",
       " 'installer',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'wpt_name',\n",
       " 'num_private',\n",
       " 'basin',\n",
       " 'subvillage',\n",
       " 'region',\n",
       " 'region_code',\n",
       " 'district_code',\n",
       " 'lga',\n",
       " 'ward',\n",
       " 'population',\n",
       " 'public_meeting',\n",
       " 'recorded_by',\n",
       " 'scheme_management',\n",
       " 'scheme_name',\n",
       " 'permit',\n",
       " 'construction_year',\n",
       " 'extraction_type',\n",
       " 'extraction_type_group',\n",
       " 'extraction_type_class',\n",
       " 'management',\n",
       " 'management_group',\n",
       " 'payment',\n",
       " 'payment_type',\n",
       " 'water_quality',\n",
       " 'quality_group',\n",
       " 'quantity',\n",
       " 'quantity_group',\n",
       " 'source',\n",
       " 'source_type',\n",
       " 'source_class',\n",
       " 'waterpoint_type',\n",
       " 'waterpoint_type_group',\n",
       " 'status_group']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_date_recorded', '_lga'] ['basin', 'region', 'scheme_management', 'extraction_type', 'extraction_type_group', 'extraction_type_class', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'quantity_group', 'source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group', 'public_meeting', 'permit']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "encoders = []\n",
    "for column in unique_columns + indexed_columns:\n",
    "    enc = StringIndexer(inputCol=column,\n",
    "                                 outputCol=\"_\"+column)\n",
    "    enc.setHandleInvalid('keep')\n",
    "    encoders.append(enc)\n",
    "\n",
    "new_indexed_columns = list(map(lambda x: \"_\"+x, indexed_columns))\n",
    "new_unique_columns = list(map(lambda x: \"_\"+x, unique_columns))\n",
    "new_new = list(map(lambda x: \"_\"+x, new_unique_columns))\n",
    "encoder2 = OneHotEncoderEstimator(inputCols=new_unique_columns,\n",
    "                                 outputCols=new_new)\n",
    "\n",
    "\n",
    "#encoder3 = OneHotEncoderEstimator(inputCols=bool_columns,\n",
    "#                                 outputCols=list(map(lambda x: \"_\"+x, bool_columns)))\n",
    "\n",
    "print(new_indexed_columns, unique_columns)\n",
    "\n",
    "label_encoder = StringIndexer(inputCol=target_column,\n",
    "                                 outputCol=label_column)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=(numeric_columns + new_new + new_indexed_columns),\n",
    "    outputCol=features_column)\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[*encoders, encoder2, label_encoder, assembler])\n",
    "\n",
    "\n",
    "model = pipeline.fit(dropped)\n",
    "encoded = model.transform(dropped)\n",
    "\n",
    "preprocessed_data = encoded.drop(*(new_unique_columns+unique_columns + bool_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+----------+---------+-----------+-----------+--------+-----------+----------+-----------+-------------+------+--------+----------+--------------------+-----------+-----------------+------------+--------------+----+--------------+-------------+-----------------------+--------------+--------------+-----------------+----------------+-------------+-------------+-------------+-----------------------+---------------+---------------+--------------+-----------------+-------------+----------------+------------------+--------------+-----------------------+-------------------+-----+--------------------+\n",
      "|amount_tsh|      date_recorded|funder|gps_height|installer|  longitude|   latitude|wpt_name|num_private|subvillage|region_code|district_code|   lga|    ward|population|         recorded_by|scheme_name|construction_year|status_group|_date_recorded|_lga|  __management|    __payment|__extraction_type_class|      __region|__source_class|__extraction_type|__public_meeting|__source_type|      __basin|   __quantity|__extraction_type_group|__quality_group|__water_quality|      __source|__waterpoint_type|     __permit|__quantity_group|__management_group|__payment_type|__waterpoint_type_group|__scheme_management|label|            features|\n",
      "+----------+-------------------+------+----------+---------+-----------+-----------+--------+-----------+----------+-----------+-------------+------+--------+----------+--------------------+-----------+-----------------+------------+--------------+----+--------------+-------------+-----------------------+--------------+--------------+-----------------+----------------+-------------+-------------+-------------+-----------------------+---------------+---------------+--------------+-----------------+-------------+----------------+------------------+--------------+-----------------------+-------------------+-----+--------------------+\n",
      "|    6000.0|2011-03-14 00:00:00| Roman|    1390.0|    Roman|34.93809275|-9.85632177|    none|        0.0|  Mnyusi B|       11.0|          5.0|Ludewa|Mundindi|     109.0|GeoData Consultan...|      Roman|           1999.0|  functional|           3.0|37.0|(12,[0],[1.0])|(7,[5],[1.0])|          (7,[0],[1.0])|(21,[0],[1.0])| (3,[0],[1.0])|   (18,[0],[1.0])|   (2,[0],[1.0])|(7,[0],[1.0])|(9,[6],[1.0])|(5,[0],[1.0])|         (13,[0],[1.0])|  (6,[0],[1.0])|  (8,[0],[1.0])|(10,[0],[1.0])|    (7,[0],[1.0])|(2,[1],[1.0])|   (5,[0],[1.0])|     (5,[0],[1.0])| (7,[5],[1.0])|          (6,[0],[1.0])|     (12,[0],[1.0])|  0.0|(183,[0,1,2,3,5,6...|\n",
      "+----------+-------------------+------+----------+---------+-----------+-----------+--------+-----------+----------+-----------+-------------+------+--------+----------+--------------------+-----------+-----------------+------------+--------------+----+--------------+-------------+-----------------------+--------------+--------------+-----------------+----------------+-------------+-------------+-------------+-----------------------+---------------+---------------+--------------+-----------------+-------------+----------------+------------------+--------------+-----------------------+-------------------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(183, {0: 6000.0, 1: 34.9381, 2: -9.8563, 3: 1390.0, 5: 11.0, 6: 5.0, 7: 109.0, 8: 1999.0, 15: 1.0, 18: 1.0, 39: 1.0, 51: 1.0, 69: 1.0, 82: 1.0, 89: 1.0, 101: 1.0, 111: 1.0, 118: 1.0, 120: 1.0, 128: 1.0, 134: 1.0, 139: 1.0, 144: 1.0, 154: 1.0, 161: 1.0, 164: 1.0, 171: 1.0, 177: 1.0, 180: 1.0, 181: 3.0, 182: 37.0}))]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.select(features_column).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59400"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.select([features_column, label_column]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "temp_df = preprocessed_data.select([label_column])\n",
    "temp_df.select([count(when(isnan(c), c)).alias(c) for c in temp_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(183,[0,1,2,3,4,5...|\n",
      "|       0.0|  2.0|(183,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(183,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(183,[0,1,2,3,4,5...|\n",
      "|       0.0|  0.0|(183,[0,1,2,3,4,5...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.285245 \n",
      "Accuracy = 0.714755 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = preprocessed_data.select([features_column, label_column]).randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=label_column, featuresCol=features_column)\n",
    "dt.setMaxBins(400)\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", label_column, features_column).show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_column, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "print(\"Accuracy = %g \" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
