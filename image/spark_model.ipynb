{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from pyspark import SparkContext, SparkConf,StorageLevel\n",
    "# create sparksession\n",
    "conf = SparkConf().setAppName(\"MLLab\").setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PLOTLY = True\n",
    "if USE_PLOTLY:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objs as go\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import plotly.io as pio\n",
    "    pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = \"../data/features.csv\"\n",
    "labels_path = \"../data/labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem description\n",
    "\n",
    "Your goal is to predict the operating condition of a waterpoint for each record in the dataset. You are provided the following set of information about the waterpoints:\n",
    "\n",
    "`amount_tsh` - Total static head (amount water available to waterpoint) <br>\n",
    "`date_recorded` - The date the row was entered<br>\n",
    "`funder` - Who funded the well<br>\n",
    "`gps_height` - Altitude of the well<br>\n",
    "`installer` - Organization that installed the well<br>\n",
    "`longitude` - GPS coordinate<br>\n",
    "`latitude` - GPS coordinate<br>\n",
    "`wpt_name` - Name of the waterpoint if there is one<br>\n",
    "`num_private` -\n",
    "`basin` - Geographic water basin<br>\n",
    "`subvillage` - Geographic location<br>\n",
    "`region` - Geographic location<br>\n",
    "`region_code` - Geographic location (coded)<br>\n",
    "`district_code` - Geographic location (coded)<br>\n",
    "`lga` - Geographic location<br>\n",
    "`ward` - Geographic location<br>\n",
    "`population` - Population around the well<br>\n",
    "`public_meeting` - True/False<br>\n",
    "`recorded_by` - Group entering this row of data<br>\n",
    "`scheme_management` - Who operates the waterpoint<br>\n",
    "`scheme_name` - Who operates the waterpoint<br>\n",
    "`permit` - If the waterpoint is permitted<br>\n",
    "`construction_year` - Year the waterpoint was constructed<br>\n",
    "`extraction_type` - The kind of extraction the waterpoint uses<br>\n",
    "`extraction_type_group` - The kind of extraction the waterpoint uses<br>\n",
    "`extraction_type_class` - The kind of extraction the waterpoint uses<br>\n",
    "`management` - How the waterpoint is managed<br>\n",
    "`management_group` - How the waterpoint is managed<br>\n",
    "`payment` - What the water costs<br>\n",
    "`payment_type` - What the water costs<br>\n",
    "`water_quality` - The quality of the water<br>\n",
    "`quality_group` - The quality of the water<br>\n",
    "`quantity` - The quantity of water<br>\n",
    "`quantity_group` - The quantity of water<br>\n",
    "`source` - The source of the water<br>\n",
    "`source_type` - The source of the water<br>\n",
    "`source_class` - The source of the water<br>\n",
    "`waterpoint_type` - The kind of waterpoint<br>\n",
    "`waterpoint_type_group` - The kind of waterpoint<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = spark.read.format(\"csv\").option(\"inferSchema\", 'true').option(\"header\", \"true\").load(features_path)\n",
    "labels = spark.read.format(\"csv\").option(\"inferSchema\", 'true').option(\"header\", \"true\").load(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_5.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_PLOTLY:\n",
    "    fig = px.histogram(labels.toPandas(), x='status_group')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'amount_tsh',\n",
       " 'date_recorded',\n",
       " 'funder',\n",
       " 'gps_height',\n",
       " 'installer',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'wpt_name',\n",
       " 'num_private',\n",
       " 'basin',\n",
       " 'subvillage',\n",
       " 'region',\n",
       " 'region_code',\n",
       " 'district_code',\n",
       " 'lga',\n",
       " 'ward',\n",
       " 'population',\n",
       " 'public_meeting',\n",
       " 'recorded_by',\n",
       " 'scheme_management',\n",
       " 'scheme_name',\n",
       " 'permit',\n",
       " 'construction_year',\n",
       " 'extraction_type',\n",
       " 'extraction_type_group',\n",
       " 'extraction_type_class',\n",
       " 'management',\n",
       " 'management_group',\n",
       " 'payment',\n",
       " 'payment_type',\n",
       " 'water_quality',\n",
       " 'quality_group',\n",
       " 'quantity',\n",
       " 'quantity_group',\n",
       " 'source',\n",
       " 'source_type',\n",
       " 'source_class',\n",
       " 'waterpoint_type',\n",
       " 'waterpoint_type_group']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "full_train = features.alias('a').join(labels.alias('b'),col('b.id') == col('a.id')).drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount_tsh=6000.0, date_recorded=datetime.datetime(2011, 3, 14, 0, 0), funder='Roman', gps_height=1390, installer='Roman', longitude=34.93809275, latitude=-9.85632177, wpt_name='none', num_private=0, basin='Lake Nyasa', subvillage='Mnyusi B', region='Iringa', region_code=11, district_code=5, lga='Ludewa', ward='Mundindi', population=109, public_meeting=True, recorded_by='GeoData Consultants Ltd', scheme_management='VWC', scheme_name='Roman', permit=False, construction_year=1999, extraction_type='gravity', extraction_type_group='gravity', extraction_type_class='gravity', management='vwc', management_group='user-group', payment='pay annually', payment_type='annually', water_quality='soft', quality_group='good', quantity='enough', quantity_group='enough', source='spring', source_type='spring', source_class='groundwater', waterpoint_type='communal standpipe', waterpoint_type_group='communal standpipe', status_group='functional')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType, DateType, BooleanType\n",
    "\n",
    "double_columns = ['amount_tsh', 'longitude', 'latitude', ]\n",
    "int_columns = ['gps_height', 'num_private', 'region_code', 'district_code', 'population','construction_year']\n",
    "bool_columns = ['public_meeting','permit']\n",
    "numeric_columns = double_columns + int_columns\n",
    "date_columns = ['date_recorded']\n",
    "\n",
    "indexed_columns = date_columns + ['lga']\n",
    "drop_colums =  ['ward', 'funder', 'installer', 'wpt_name', 'subvillage', 'scheme_name', 'recorded_by']\n",
    "\n",
    "target_column = 'status_group'\n",
    "\n",
    "not_unique_columns = double_columns + int_columns + bool_columns + drop_colums + [target_column] + indexed_columns\n",
    "unique_columns = [elem for elem in full_train.columns if elem not in not_unique_columns] + bool_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_10.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_PLOTLY:\n",
    "    import plotly.express as px\n",
    "    fig = px.scatter_matrix(full_train.toPandas()[numeric_columns])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = full_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numeric_columns:\n",
    "    dropped =  dropped.withColumn(column, dropped[column].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in bool_columns:\n",
    "    dropped =  dropped.withColumn(column, dropped[column].cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in date_columns:\n",
    "    dropped =  dropped.withColumn(column, dropped[column].cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_columns:\n",
      "basin 9\n",
      "region 21\n",
      "scheme_management 13\n",
      "extraction_type 18\n",
      "extraction_type_group 13\n",
      "extraction_type_class 7\n",
      "management 12\n",
      "management_group 5\n",
      "payment 7\n",
      "payment_type 7\n",
      "water_quality 8\n",
      "quality_group 6\n",
      "quantity 5\n",
      "quantity_group 5\n",
      "source 10\n",
      "source_type 7\n",
      "source_class 3\n",
      "waterpoint_type 7\n",
      "waterpoint_type_group 6\n",
      "public_meeting 3\n",
      "permit 3\n",
      "\n",
      "dropped columns:\n",
      "ward 2092\n",
      "funder 1898\n",
      "installer 2146\n",
      "wpt_name 37400\n",
      "subvillage 19288\n",
      "scheme_name 2697\n",
      "recorded_by 1\n",
      "\n",
      "indexed columns:\n",
      "date_recorded 356\n",
      "lga 125\n"
     ]
    }
   ],
   "source": [
    "unique_values = {}\n",
    "print(\"unique_columns:\")\n",
    "for column in unique_columns:\n",
    "    unique_values[column] = dropped.select(column).distinct().count()\n",
    "    print(column, unique_values[column])\n",
    "print('\\ndropped columns:')\n",
    "for column in drop_colums:\n",
    "    unique_values[column] = full_train.select(column).distinct().count()\n",
    "    print(column, unique_values[column])\n",
    "print('\\nindexed columns:')\n",
    "for column in indexed_columns:\n",
    "    unique_values[column] = full_train.select(column).distinct().count()\n",
    "    print(column, unique_values[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(amount_tsh,DoubleType,true),StructField(date_recorded,StringType,true),StructField(funder,StringType,true),StructField(gps_height,DoubleType,true),StructField(installer,StringType,true),StructField(longitude,DoubleType,true),StructField(latitude,DoubleType,true),StructField(wpt_name,StringType,true),StructField(num_private,DoubleType,true),StructField(basin,StringType,true),StructField(subvillage,StringType,true),StructField(region,StringType,true),StructField(region_code,DoubleType,true),StructField(district_code,DoubleType,true),StructField(lga,StringType,true),StructField(ward,StringType,true),StructField(population,DoubleType,true),StructField(public_meeting,StringType,true),StructField(recorded_by,StringType,true),StructField(scheme_management,StringType,true),StructField(scheme_name,StringType,true),StructField(permit,StringType,true),StructField(construction_year,DoubleType,true),StructField(extraction_type,StringType,true),StructField(extraction_type_group,StringType,true),StructField(extraction_type_class,StringType,true),StructField(management,StringType,true),StructField(management_group,StringType,true),StructField(payment,StringType,true),StructField(payment_type,StringType,true),StructField(water_quality,StringType,true),StructField(quality_group,StringType,true),StructField(quantity,StringType,true),StructField(quantity_group,StringType,true),StructField(source,StringType,true),StructField(source_type,StringType,true),StructField(source_class,StringType,true),StructField(waterpoint_type,StringType,true),StructField(waterpoint_type_group,StringType,true),StructField(status_group,StringType,true)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amount_tsh': 0.0, 'longitude': 34.90038636, 'latitude': -5.02762847, 'gps_height': 365.0, 'num_private': 0.0, 'region_code': 12.0, 'district_code': 3.0, 'population': 25.0, 'construction_year': 1986.0}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "def fill_with_mean(df, include=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in include\n",
    "    ))\n",
    "    return df.fillna(stats.first().asDict())\n",
    "\n",
    "def fill_with_median(df, include=list()):\n",
    "    medians = df.approxQuantile(include, [0.5], 0.01)\n",
    "    assert isinstance(include, (tuple,list))\n",
    "    medians = list(map(lambda x: x[0],medians))\n",
    "    medians=dict(zip(include,medians))\n",
    "    print(medians)\n",
    "    return df.fillna(medians)\n",
    "\n",
    "\n",
    "dropped = fill_with_median(dropped, numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_column = \"features\"\n",
    "label_column = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(amount_tsh=6000.0, longitude=34.93809275, latitude=-9.85632177, gps_height=1390.0, num_private=0.0, region_code=11.0, district_code=5.0, population=109.0, construction_year=1999.0),\n",
       " Row(amount_tsh=0.0, longitude=34.6987661, latitude=-2.14746569, gps_height=1399.0, num_private=0.0, region_code=20.0, district_code=2.0, population=280.0, construction_year=2010.0),\n",
       " Row(amount_tsh=25.0, longitude=37.46066446, latitude=-3.82132853, gps_height=686.0, num_private=0.0, region_code=21.0, district_code=4.0, population=250.0, construction_year=2009.0),\n",
       " Row(amount_tsh=0.0, longitude=38.48616088, latitude=-11.15529772, gps_height=263.0, num_private=0.0, region_code=90.0, district_code=63.0, population=58.0, construction_year=1986.0),\n",
       " Row(amount_tsh=0.0, longitude=31.13084671, latitude=-1.82535885, gps_height=0.0, num_private=0.0, region_code=18.0, district_code=1.0, population=0.0, construction_year=0.0)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.select(numeric_columns).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|status_group|\n",
      "+------------+\n",
      "|           0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropped.schema\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "temp_df = dropped.select([target_column])\n",
    "temp_df.select([count(when(isnan(c), c)).alias(c) for c in temp_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount_tsh',\n",
       " 'date_recorded',\n",
       " 'funder',\n",
       " 'gps_height',\n",
       " 'installer',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'wpt_name',\n",
       " 'num_private',\n",
       " 'basin',\n",
       " 'subvillage',\n",
       " 'region',\n",
       " 'region_code',\n",
       " 'district_code',\n",
       " 'lga',\n",
       " 'ward',\n",
       " 'population',\n",
       " 'public_meeting',\n",
       " 'recorded_by',\n",
       " 'scheme_management',\n",
       " 'scheme_name',\n",
       " 'permit',\n",
       " 'construction_year',\n",
       " 'extraction_type',\n",
       " 'extraction_type_group',\n",
       " 'extraction_type_class',\n",
       " 'management',\n",
       " 'management_group',\n",
       " 'payment',\n",
       " 'payment_type',\n",
       " 'water_quality',\n",
       " 'quality_group',\n",
       " 'quantity',\n",
       " 'quantity_group',\n",
       " 'source',\n",
       " 'source_type',\n",
       " 'source_class',\n",
       " 'waterpoint_type',\n",
       " 'waterpoint_type_group',\n",
       " 'status_group']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_date_recorded', '_lga'] ['basin', 'region', 'scheme_management', 'extraction_type', 'extraction_type_group', 'extraction_type_class', 'management', 'management_group', 'payment', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'quantity_group', 'source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group', 'public_meeting', 'permit']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, StandardScaler as SC\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "encoders = []\n",
    "\n",
    "assembler_numeric = VectorAssembler(\n",
    "    inputCols=numeric_columns,\n",
    "    outputCol='numeric')\n",
    "scaler = SC(inputCol='numeric', outputCol='_'+'numeric',\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "\n",
    "    \n",
    "for column in unique_columns + indexed_columns:\n",
    "    enc = StringIndexer(inputCol=column,\n",
    "                                 outputCol=\"_\"+column)\n",
    "    \n",
    "    enc.setHandleInvalid('keep')\n",
    "    encoders.append(enc)\n",
    "\n",
    "new_indexed_columns = list(map(lambda x: \"_\"+x, indexed_columns))\n",
    "new_unique_columns = list(map(lambda x: \"_\"+x, unique_columns))\n",
    "new_new = list(map(lambda x: \"_\"+x, new_unique_columns))\n",
    "encoder2 = OneHotEncoderEstimator(inputCols=new_unique_columns,\n",
    "                                 outputCols=new_new)\n",
    "\n",
    "\n",
    "#encoder3 = OneHotEncoderEstimator(inputCols=bool_columns,\n",
    "#                                 outputCols=list(map(lambda x: \"_\"+x, bool_columns)))\n",
    "\n",
    "print(new_indexed_columns, unique_columns)\n",
    "\n",
    "label_encoder = StringIndexer(inputCol=target_column,\n",
    "                                 outputCol=label_column)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=(['_numeric'] + new_new + new_indexed_columns),\n",
    "    outputCol=features_column)\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler_numeric, scaler, *encoders, encoder2, label_encoder, assembler])\n",
    "\n",
    "\n",
    "model = pipeline.fit(dropped)\n",
    "encoded = model.transform(dropped)\n",
    "\n",
    "preprocessed_data = encoded.drop(*(new_unique_columns+unique_columns + bool_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(183, {0: 1.8956, 1: 0.1311, 2: -1.4088, 3: 1.0412, 4: -0.0387, 5: -0.2443, 6: -0.0654, 7: -0.1504, 8: 0.7339, 15: 1.0, 18: 1.0, 39: 1.0, 51: 1.0, 69: 1.0, 82: 1.0, 89: 1.0, 101: 1.0, 111: 1.0, 118: 1.0, 120: 1.0, 128: 1.0, 134: 1.0, 139: 1.0, 144: 1.0, 154: 1.0, 161: 1.0, 164: 1.0, 171: 1.0, 177: 1.0, 180: 1.0, 181: 3.0, 182: 37.0}))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.select(features_column).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59400"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.select([features_column, label_column]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "temp_df = preprocessed_data.select([label_column])\n",
    "temp_df.select([count(when(isnan(c), c)).alias(c) for c in temp_df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vis projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.89564913e+00  1.31050627e-01 -1.40877876e+00 ...  1.00000000e+00\n",
      "   3.00000000e+00  3.70000000e+01]\n",
      " [-1.05969136e-01  9.46091901e-02  1.20792394e+00 ...  0.00000000e+00\n",
      "   1.88000000e+02  2.20000000e+01]\n",
      " [-9.76290594e-02  5.15153845e-01  6.39746070e-01 ...  0.00000000e+00\n",
      "   9.10000000e+01  8.10000000e+01]\n",
      " ...\n",
      " [-1.05969136e-01 -9.18770586e-03 -1.03339510e+00 ...  1.00000000e+00\n",
      "   1.66000000e+02  3.20000000e+01]\n",
      " [-1.05969136e-01  2.71626514e-01 -2.28287934e-01 ...  0.00000000e+00\n",
      "   6.40000000e+01  7.30000000e+01]\n",
      " [-1.05969136e-01  6.13119652e-01 -3.53504700e-01 ...  0.00000000e+00\n",
      "   1.80000000e+01  4.20000000e+01]]\n",
      "(59400, 183)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_PLOTLY:\n",
    "    import numpy as np\n",
    "    vis_X  = np.stack(preprocessed_data.select([features_column]).toPandas()[features_column].apply(lambda x : np.array(x.toArray())).values)\n",
    "    print(vis_X)\n",
    "    print(vis_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PLOTLY:\n",
    "    vis_Y  = np.stack(preprocessed_data.select([label_column]).toPandas()[label_column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.28474841 -0.14029904]\n",
      " [ 1.41110138 -0.47928638]\n",
      " [-0.01934886  1.30990496]\n",
      " ...\n",
      " [ 1.08781893 -0.17911527]\n",
      " [-0.4092007   1.03904494]\n",
      " [-1.06814168  0.02932886]]\n",
      "[[-1.28474841 -0.14029904]\n",
      " [ 1.41110138 -0.47928638]\n",
      " [-0.01934886  1.30990496]\n",
      " ...\n",
      " [ 1.08781893 -0.17911527]\n",
      " [-0.4092007   1.03904494]\n",
      " [-1.06814168  0.02932886]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_30.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if USE_PLOTLY:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    X_embedded = StandardScaler().fit_transform(PCA(n_components=2).fit_transform(vis_X))\n",
    "    print(X_embedded)\n",
    "    fig = px.scatter(x=X_embedded[:,0], y=X_embedded[:,1], color=vis_Y)\n",
    "    print(X_embedded)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  1.0|(183,[0,1,2,3,4,5...|\n",
      "|       1.0|  1.0|(183,[0,1,2,3,4,5...|\n",
      "|       0.0|  1.0|(183,[0,1,2,3,4,5...|\n",
      "|       1.0|  0.0|(183,[0,1,2,3,4,5...|\n",
      "|       1.0|  0.0|(183,[0,1,2,3,4,5...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.280792 \n",
      "Accuracy = 0.719208 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and test sets (20% held out for testing)\n",
    "(trainingData, testData) = preprocessed_data.select([features_column, label_column]).randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=label_column, featuresCol=features_column)\n",
    "dt.setMaxBins(400)\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", label_column, features_column).show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_column, predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "print(\"Accuracy = %g \" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
